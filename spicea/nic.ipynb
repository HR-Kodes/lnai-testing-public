{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Read the dataset into a DataFrame\n",
    "df_nic = pd.read_excel('NIC.xlsx')\n",
    "df_nic=df_nic.dropna(subset=\"Sub-class\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to convert output to PyTorch tensors format, PyTorch is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m user_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtax legal information services\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute token embeddings for the user input\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2577\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2574'>2575</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2575'>2576</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2576'>2577</a>\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2577'>2578</a>\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2578'>2579</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2683\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2662'>2663</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2663'>2664</a>\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2664'>2665</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2679'>2680</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2680'>2681</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2681'>2682</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2682'>2683</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2683'>2684</a>\u001b[0m         text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2684'>2685</a>\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2685'>2686</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2686'>2687</a>\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2687'>2688</a>\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2688'>2689</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2689'>2690</a>\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2690'>2691</a>\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2691'>2692</a>\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2692'>2693</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2693'>2694</a>\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2694'>2695</a>\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2695'>2696</a>\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2696'>2697</a>\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2697'>2698</a>\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2698'>2699</a>\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2699'>2700</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2700'>2701</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2701'>2702</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2756\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2745'>2746</a>\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2746'>2747</a>\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2747'>2748</a>\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2748'>2749</a>\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2752'>2753</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2753'>2754</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2755'>2756</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2756'>2757</a>\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2757'>2758</a>\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2758'>2759</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2759'>2760</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2760'>2761</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2761'>2762</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2762'>2763</a>\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2763'>2764</a>\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2764'>2765</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2765'>2766</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2766'>2767</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2767'>2768</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2768'>2769</a>\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2769'>2770</a>\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2770'>2771</a>\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2771'>2772</a>\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2772'>2773</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2773'>2774</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2774'>2775</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:497\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=474'>475</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=475'>476</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=476'>477</a>\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=493'>494</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=494'>495</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=495'>496</a>\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=496'>497</a>\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=497'>498</a>\u001b[0m         batched_input,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=498'>499</a>\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=499'>500</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=500'>501</a>\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=501'>502</a>\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=502'>503</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=503'>504</a>\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=504'>505</a>\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=505'>506</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=506'>507</a>\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=507'>508</a>\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=508'>509</a>\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=509'>510</a>\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=510'>511</a>\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=511'>512</a>\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=512'>513</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=513'>514</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=514'>515</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=516'>517</a>\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=517'>518</a>\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=518'>519</a>\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:473\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=470'>471</a>\u001b[0m \u001b[39mfor\u001b[39;00m input_ids \u001b[39min\u001b[39;00m sanitized_tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=471'>472</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=472'>473</a>\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:211\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=206'>207</a>\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=208'>209</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=210'>211</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:700\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=697'>698</a>\u001b[0m \u001b[39melif\u001b[39;00m tensor_type \u001b[39m==\u001b[39m TensorType\u001b[39m.\u001b[39mPYTORCH:\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=698'>699</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_available():\n\u001b[0;32m--> <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=699'>700</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to convert output to PyTorch tensors format, PyTorch is not installed.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=700'>701</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pratik/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=702'>703</a>\u001b[0m     as_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to convert output to PyTorch tensors format, PyTorch is not installed."
     ]
    }
   ],
   "source": [
    "# Tokenize and preprocess the user input\n",
    "# user_input = \"Develop processes for health care solutions, sustainable solutions, Affordable solutions, enabling the deskilled, Inclusive solutions FOR ALL strata of humanity\"\n",
    "user_input=\"tax legal information services\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "encoded_input = tokenizer(user_input, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings for the user input\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "user_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "user_embedding = F.normalize(user_embedding, p=2, dim=1)\n",
    "\n",
    "# Compute token embeddings for each row in the \"Description\" column\n",
    "encoded_descriptions = tokenizer(df_nic['Description'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    descriptions_output = model(**encoded_descriptions)\n",
    "description_embeddings = mean_pooling(descriptions_output, encoded_descriptions['attention_mask'])\n",
    "description_embeddings = F.normalize(description_embeddings, p=2, dim=1)\n",
    "\n",
    "# Compute cosine similarity between the user embedding and each description embedding\n",
    "similarity_scores = F.cosine_similarity(user_embedding, description_embeddings)\n",
    "\n",
    "# Find the indices of the top 3 most similar descriptions\n",
    "# best_match_indices = similarity_scores.argsort()[::-1][:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
